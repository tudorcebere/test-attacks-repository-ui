URL: https://proceedings.neurips.cc/paper/2020/file/fc4ddc15f9f4b4b06ef7844d6bb53abf-Paper.pdf
BibTex (Please add a bibtex entry for this paper to facilitate easy citations): "@article{jagielski2020auditing,\n\
  \  title={Auditing differentially private machine learning: How private is private\
  \ {SGD}?},\n  author={Jagielski, Matthew and Ullman, Jonathan and Oprea, Alina},\n\
  \  journal={Advances in Neural Information Processing Systems},\n}"
Authors: Matthew Jagielski, Jonathan Ullman, Alina Oprea
Title: 'Auditing Differentially Private Machine Learning:

  How Private is Private SGD?'
Short Description: Connects the success rate of a membership inferency adversary to
  a lower bound on the privacy loss of the underlying DP mechanism.
Data Type: Image
Type of Release: Predictive-Model
Threat Model --- Attacker Objective: Membership-Inference
Threat Model --- Attacker Capabilities: .nan
Research Type: Empirical
Links to Artifacts: .nan
Comments: .nan
Submitter (your name, affiliation): Tudor Cebere, Inria
